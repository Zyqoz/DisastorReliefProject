---
title: 'Disaster Relief Project: Part 1'
author: "Part 1"
output:
  html_document:
    toc: no
    df_print: paged
  pdf_document:
    toc: no
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

==

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5.5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=TRUE,      # show R warnings
                      message=TRUE)      # show R messages
```

<!--- Change font sizes (or other css modifications) --->

```{=html}
<style>
h1.title { font-size: 2.2em; }
h1 { font-size: 2em; }
h2 { font-size: 1.5em; }
h3 { font-size: 1.2em; }
pre { font-size: 0.8em; }
</style>
```
**DS 6030 \| Spring 2023 \| University of Virginia**

------------------------------------------------------------------------

In this project, you will use classification methods covered in this course to solve a real historical data-mining problem: locating displaced persons living in makeshift shelters following the destruction of the earthquake in Haiti in 2010.

Following that earthquake, rescue workers, mostly from the United States military, needed to get food and water to the displaced persons. But with destroyed communications, impassable roads, and thousands of square miles, actually locating the people who needed help was challenging.

As part of the rescue effort, a team from the Rochester Institute of Technology were flying an aircraft to collect high resolution geo-referenced imagery. It was known that the people whose homes had been destroyed by the earthquake were creating temporary shelters using blue tarps, and these blue tarps would be good indicators of where the displaced persons were -- if only they could be located in time, out of the thousands of images that would be collected every day. The problem was that there was no way for aid workers to search the thousands of images in time to find the tarps and communicate the locations back to the rescue workers on the ground in time. The solution would be provided by data-mining algorithms, which could search the images far faster and more thoroughly (and accurately?) then humanly possible. The goal was to find an algorithm that could effectively search the images in order to locate displaced persons and communicate those locations rescue workers so they could help those who needed it in time.

This disaster relief project is the subject matter for your project in this course, which you will submit in two parts. You will use data from the actual data collection process was carried out over Haiti. Your goal is to test each of the algorithms you learn in this course on the imagery data collected during the relief efforts made Haiti in response to the 2010 earthquake, and determine which method you will use to as accurately as possible, and in as timely a manner as possible, locate as many of the displaced persons identified in the imagery data so that they can be provided food and water before their situations become unsurvivable.

You will document the performance of several models using cross-validation (Part I) and a hold-out testing set (Part II). In **Module 6** you will submit the results for Part I that includes performance of the models we have covered in Modules 1-5. In **Module 12** you will submit the results for Part II that includes performance of a few other models, overall conclusions, and recommendations on the preferred model for this application.

## Submission Format

You will submit **both an Rmarkdown (.Rmd) and compiled .pdf document**. <!-- A template Rmarkdown file will be provided. The template is a suggestion to get you started.
You are completely free to modify the format; donâ€™t let it constrain your creativity. Also, the requirements list below are minimum requirements, you can provide more than what is asked (e.g., interactive shiny app). -->

## Collaboration and Help

-   While all work must be your own, you are permitted to discuss this project with classmates and post questions and answers on the discussion boards (e.g., slack).
    -   However, you are **not** permitted to work collaboratively.
-   You are not permitted to copy code. You will no doubt come across examples on the internet. You can consult them to help understand the concept or process, but *code in your own words*.
-   It is a scholarly responsibility to attribute all your work. This includes figures, code, ideas, etc. Think of it this way: will someone who reads your submission think that it is your original idea, figure, code, etc? Add a link and/or reference to all sources you used to solve a problem. It is really of no value to you when you just copy someone else's solutions (other then preserve a grade that you didn't earn). It is not always easy to tell what qualifies as an honor code violation, so do not be afraid to talk to me about it. Such discussions do not imply guilt of any kind.

# Project Part I (100 points) DUE in Module 6

Use 10-fold cross-validation to evaluate the performance of 5 models:

-   Logistic Regression
-   LDA (Linear Discriminant Analysis)
-   QDA (Quadratic Discriminant Analysis)
-   KNN (K-nearest neighbor)
-   Penalized Logistic Regression (elastic net penalty)

Use the `HaitiPixels.csv` data (provided in Module 3).

## Document Format (5pts)

Compiled document well structured and easy to read:

-   organized well
    -   the provided template should satisfy the minimal requirements
    -   (optional) make it better (e.g., used tabbed sections, custom css, different themes)
-   tables/plots fit on the page
-   there are not extraneous outputs (e.g., printing a matrix that fills 2 pages)
-   plots are labeled correctly
-   etc.

## Coding (5 pts)

All code is shown and executes without errors. The R code in the code chunks should be visible and easy to follow. Use `echo = TRUE` for all chunks that were actually used (e.g., personal notes to yourself or preliminary coding attempts shouldn't be shown).

-   Alternatively, you can show all of the code you used at the end of the document in an appendix.

We will **only** inspect your .Rmd file if there are problems in compiling your document so ensure we can understand what you implemented from the compiled document.

## Data Wrangling and EDA (10 pts)

Data loaded correctly and exploratory data anlysis (EDA) is performed to better understand the data

## Model Fitting, Tuning Parameter Selection, and Evaluation (30 pts)

-   Overall model building process well defined and *explained*.
    -   describe and justify parameter tuning and model selection (if applicable)
    -   describe and justify model validation
    -   describe and justify threshold selection
        -   It should be clear how the threshold was applied (e.g., to the estimated probabilities or logit of the probabilities).
    -   describe and justify metrics used for model performance evaluation
        -   use ROC curves to compare models
    -   It should be clear what features were used for each model family.
        -   E.g., by using a formula or explicit calculation of model matrix.
    -   It should be clear if any pre-processing was used (e.g., scaling).
    -   It should be clear how cross-validation was implemented.
-   For each of the 5 models,
    -   describe and show parameter tuning and discuss results (use tables and/or plots)
    -   describe and show results of threshold selection
    -   describe and discuss model performance
        -   use ROC curves and relevant metrics; how are they derived

## Performance Table (20 pts)

Model performance summarized in one or more tables. Expected information shown:

-   Optimal model tuning parameters

-   AUC

-   Selected threshold

-   Accuracy, TPR, FPR, Precision calculated at selected threshold

-   Describe how the metrics were calculated under the cross-validation framework.

    -   E.g., is it an average, a sum, a max, etc.

## Conclusions (30 pts)

Report on at least 3 conclusions. This section is more important than the previous sections (as reflected in the points). Give sufficient explanation and justification for each conclusion.

-   One must be your **determination and justification of which algorithm works best**
-   Additional conclusions should be observations you've made based on your work on this project, such as:
    -   Examples:

        -   What additional recommend actions can be taken to improve results?
        -   Were there multiple adequately performing methods, or just one clear best method? What is your level of confidence in the results?
        -   What is it about this data formulation that allows us to address it with predictive modeling tools?
        -   How effective do you think your work here could actually be in terms of helping to save human life?
        -   Do these data seem particularly well-suited to one class of prediction methods, and if so, why?

    -   These are only suggestions, pursue your own interests.

    -   Your *best two additional* conclusions will be graded.

# Project

#### Downloading libraries

```{r, cache = TRUE, echo = TRUE}
library(caret)
library(ROCR)
library(tidyverse)
```

## Data Wrangling and Exploratory Data Analysis

```{r, cache = TRUE, echo = TRUE}

data <- read.csv('HaitiPixels.csv')
data <- na.omit(data)
install.packages("glmnet")
```

```{r, cache = TRUE, echo = TRUE}

#Box plots
ggplot(data, aes(x = Class, y = Red))+
  geom_boxplot()+
  ggtitle("Boxplot of Class and Red color")+
  theme(plot.title = element_text(hjust = 0.5))
ggplot(data, aes(x = Class, y = Blue))+
  geom_boxplot()+
  ggtitle("Boxplot of Class and Blue color")+
  theme(plot.title = element_text(hjust = 0.5))
ggplot(data, aes(x = Class, y = Green))+
  geom_boxplot()+
  ggtitle("Boxplot of Class and Green color")+
  theme(plot.title = element_text(hjust = 0.5))
```

Pixels that are classified as Blue Tarp have a median of about 170 red, 180 green, and 220 blue channel values. Furthermore, the distribution of blue channel values for Blue tarp is higher than red or green channel values for Blue Tarp as it should be expected. This means that when looking at images of Blue Tarps, we should expect to see more blue channel values in the area of the Blue Tarp.

```{r, cache = TRUE, echo = TRUE}
ggplot(data, aes(x = Class)) + 
  geom_bar() +
  labs(y = "Count")+
  ggtitle("Count of Each Class in the Entire Dataset")+
  theme(plot.title = element_text(hjust = 0.5))


```

Most of this data set has pixels that are classified as Vegetation followed by Soil. The class with the least pixels is the Blue Tarp class. This means that for this image, only a small portion of the image has a tarp visible.

```{r, cache = TRUE, echo = TRUE}

# Create a new dataframe with the percentages of each color for each class
data_perc <- data %>% 
  group_by(Class) %>% 
  summarise(Red = mean(Red),
            Blue = mean(Blue),
            Green = mean(Green))

# Reshape the data to long format
data_long <- data_perc %>% 
  pivot_longer(cols = Red:Green, names_to = "Color", values_to = "Percentage")

# Create a named vector of colors
color_vector <- c("Red" = "red", "Blue" = "blue", "Green" = "green")

# Create a stacked bar plot for each class
ggplot(data_long, aes(x = Class, y = Percentage, fill = Color)) +
  geom_bar(position = "fill", stat = "identity") +
  scale_fill_manual(values = color_vector) +
  labs(x = "Class", y = "Proportion", fill = "Color") +
  theme_bw()+
  ggtitle("Proportion of Color for Each Class")+
  theme(plot.title = element_text(hjust = 0.5))
```

This visualization shows the average color channel values for each class. From this, we can see that the Blue Tarp class has the greatest percentage of blue-channel values. This may be of use when we are trying to determine if some pixels contain a blue tarp, as those pixels with the highest blue channel values will likely be Blue Tarps.

## Logistic Regression

The data is modified so that logistic regression can be applied on it with a new column called Blue Tarp which is equal to "Yes" if the pixel was classified as a Blue Tarp and "No" if it was not. A general logistic regression model with the output being the newly created column is fitted. The training split is done so that 80% of the data is used for training and 20% is for the hold out testing. Then the cross validation approach is used to see if there is over fitting in the data or not with 10 folds. Model parameters are printed. There is a loop done to see which threshold value gives the best accuracy. The threshold is applied to the estimated probabilities. The ROC curve is plotted and the AUC value is calculated and these metrics are used for model evaluation along with the mean accuracy of the model over all cross validation folds, and max accuracy calculated on the hold-out set. No pre-processing was done and the predictors were Red, Blue, and Green. The precision value, TRUE-positive rate, and false-positive rate is also presented. All the model metrics are presented in a single data frame at the end.

#### Training and Cross-Validation

```{r, cache = TRUE, echo = TRUE}

#Fit a logistic regression model and do cv on it to see if it overfitting or not.

#Modify the data to add a new column for Blue Tarp

data$Blue_Tarp <- as.factor(ifelse(data$Class == "Blue Tarp", 1, 0))

#Splitting data for a training set and a holdout set
train.index <- sample(1:nrow(data), 0.8*nrow(data), replace = FALSE)
train.data <- data[train.index, ]
test.data <- data[-train.index, ]

#Set a seed
set.seed(123)

# Define cross-validation control
ctrl <- trainControl(method = "cv", number = 10, returnResamp = "all")

# Train model with cross-validation
model <- train(Blue_Tarp ~ Red + Blue + Green, data = train.data, method = "glm",
               family = binomial(link = "logit"), trControl = ctrl, metric = "Accuracy")

# Print model results
print(model$finalModel)

accuracies <- model$resample$Accuracy

# Create a bar chart of the accuracies
barplot(accuracies, main = "Model Accuracies", xlab = "Resampling Iteration", ylab = "Accuracy")

meanAccuracy <- mean(accuracies)
cat("Mean accuracy over all cross-validation folds:", meanAccuracy, "\n")

```

------------------------------------------------------------------------

**Comment:** When plotting the accuracy of the model over each cross-validation fold, we see that the accuracy does not change significantly. This means that the model has not been over fit to any specific fold. The mean accuracy over all the folds was 99.5%.

#### ROC Curve

```{r, cache = TRUE, echo = TRUE}

preds <- predict(model$finalModel, newdata = test.data, type="response")

rates <- prediction(preds, test.data$Blue_Tarp)


roc_result <- performance(rates, measure = "tpr", x.measure = "fpr")

plot(roc_result, main="ROC Curve for Blue Tarp Classification")
lines(x = c(0,1), y = c(0,1), col="red")
```

**Comment:** Since the ROC curve for this model is far above the diagonal line, this means the model performs much better than random guessing and is thus a useful model.

#### AUC Value

```{r, cache = TRUE, echo = TRUE}

auc <- performance(rates, measure = "auc")

aucValue <- as.numeric(auc@y.values[1])

cat("AUC:", aucValue, "\n")

```

**Comment:** Since the AUC of this model is 0.999 and is much greater than 0.5, this model performs much better than random guessing.

#### Confusion Matrix

```{r, cache = TRUE, echo = TRUE}

modelAccuracies <- numeric(length = 11)

for (x in seq(0, 1, 0.1)) {
	
	# Convert the predictions to binary classes
	predictions_class <- ifelse(preds > x, 1, 0)
	
	# Create the confusion matrix
	
	Actual <- test.data$Blue_Tarp
	Predicted <- predictions_class
	
  cm <- table(Predicted, Actual)
  modelAccuracies[10 * x + 1] <- sum(diag(cm))/sum(cm)
}

plot(modelAccuracies, main = "Model Accuracies at Different Thresholds", xlab = "Threshold Value", ylab = "Accuracy")

max_idx <- which.max(modelAccuracies)
bestThresholdValue <- (max_idx - 1) / 10
```

**Comment:** Threshold selection was done on the model with increments of 0.1 going from 0 to 1. The highest accuracy was obtained with a threshold value of 0.3 at an accuracy of 0.995. Threshold selection was applied to the estimated probabilities.

#### Confusion Matrix and Best Threshold Metrics

```{r, cache = TRUE, echo = TRUE}

print("Confusion matrix at best threshold value")
cm <- table(test.data$Blue_Tarp, preds > bestThresholdValue)
cm
# Calculate accuracy
accuracy <- sum(diag(cm))/sum(cm)
# Calculate error rate
error_rate <- 1 - accuracy

# Extract the TRUE positives (TP), false positives (FP), TRUE negatives (TN), and false negatives (FN) from the confusion matrix
TP <- cm[2,2]
FP <- cm[1,2]
TN <- cm[1,1]
FN <- cm[2,1]

# Calculate the TRUE positive rate (TPR)
TPR <- TP / (TP + FN)

# Calculate the false positive rate (FPR)
FPR <- FP / (FP + TN)

# Calculate precision
precision <- TP / (TP + FP)

#Putting TPR,FPR, aucValue, bestThresholdValue, precision, and accuracy into a dataframe

logResults <- data.frame(
  TPR = TPR,
  FPR = FPR,
  AUC = aucValue,
  Best_Threshold_Value = bestThresholdValue,
  Precision = precision,
  Max_Accuracy = accuracy,
  Mean_Accuracy = meanAccuracy
)

print(t(logResults))
```

## LDA (Linear Discriminant Analysis)

A general linear discriminant analysis model with the response variable being the Blue_Tarp created column is fitted. The training split is done so that 80% of the data is used for training and 20% is for the hold out testing. Then the cross validation approach is used to see if there is over fitting in the data or not with 10 folds. Model parameters are printed. There is a loop done to see which threshold value gives the best accuracy. The threshold is applied to the estimated probabilities. The ROC curve is plotted and the AUC value is calculated and these metrics are used for model evaluation along with the mean accuracy of the model over all cross validation folds, and max accuracy calculated on the hold-out set. No pre-processing was done and the predictors were Red, Blue, and Green. The precision value, TRUE-positive rate, and false-positive rate is also presented. All the model metrics are presented in a single data frame at the end.

### Training and Performing Cross-validation

```{r, cache = TRUE, echo = TRUE}

# Define cross-validation control
ctrl <- trainControl(method = "cv", number = 10, returnResamp = "all")

# Train model with cross-validation
model <- train(Blue_Tarp ~ Red + Blue + Green, data = train.data, method = "lda", trControl = ctrl, metric = "Accuracy")

# Print model results
print(model$finalModel)

accuracies <- model$resample$Accuracy

# Create a bar chart of the accuracies
barplot(accuracies, main = "Model Accuracies", xlab = "Resampling Iteration", ylab = "Accuracy")

meanAccuracy <- mean(accuracies)
cat("Mean accuracy over all cross-validation folds:", meanAccuracy, "\n")


```

**Comment:** When plotting the accuracy of the model over each cross-validation fold, we see that the accuracy does not change significantly. This means that the model has not been over fit to any specific fold. The mean accuracy over all the folds was 99.8%.

#### ROC Curve

```{r, cache = TRUE, echo = TRUE}

# Load the MASS library
library(MASS)


# Fit the LDA model using the training data
model_training <- lda(Blue_Tarp ~ Red + Blue + Green, data = train.data)

# Make predictions on the test data
predictions_test <- predict(model_training, newdata = test.data, type = "response")
predictions_class <- predictions_test$class

# Create the confusion matrix
Actual <- test.data$Blue_Tarp
Predicted <- predictions_class
table <- table(Predicted, Actual)

# Print the confusion matrix
cat("Confusion Matrix:\n\n")
print(table)

# Calculate the overall fraction of correct predictions
accuracy <- sum(diag(table))/sum(table)

# Print the accuracy
cat("\nAccuracy:", accuracy, "\n")

preds <- predict(model_training, newdata = test.data, type = "response")

prediction_obj <- prediction(preds$posterior[,2], test.data$Blue_Tarp)

roc_obj <- performance(prediction_obj, measure = "tpr", x.measure = "fpr")
plot(roc_obj, main="ROC Curve for Blue Tarp Classification")
lines(x = c(0,1), y = c(0,1), col="red")

```

**Comment:** Since the ROC curve for this model is far above the diagonal line, this means the model performs much better than random guessing and is thus a useful model.

#### AUC Value

```{r, cache = TRUE, echo = TRUE}

auc <- performance(prediction_obj, measure = "auc")

aucValue <- as.numeric(auc@y.values[1])

cat("AUC:", aucValue, "\n")

```

**Comment:** Since the AUC of this model is 0.9884393 and is much greater than 0.5, this model performs much better than random guessing.

#### Confusion Matrix

```{r, cache = TRUE, echo = TRUE}

modelAccuracies <- numeric(length = 11)
for (x in seq(0, 1, 0.1)) {
	
	# Convert the predictions to binary classes
	predictions_class <- ifelse(preds$posterior[,2] > x, 1, 0)
	
	# Create the confusion matrix
	
	Actual <- test.data$Blue_Tarp
	Predicted <- predictions_class
	
  cm <- table(Predicted, Actual)
  modelAccuracies[10 * x + 1] <- sum(diag(cm))/sum(cm)
}

plot(modelAccuracies, main = "Model Accuracies at Different Thresholds", xlab = "Threshold Value", ylab = "Accuracy")

max_idx <- which.max(modelAccuracies)
bestThresholdValue <- (max_idx - 1) / 10

```

**Comment:** Threshold selection was done on the model with increments of 0.1 going from 0 to 1. Threshold selection was applied to the estimated probabilities.

#### Confusion Matrix and Best Threshold Metrics

```{r, cache = TRUE, echo = TRUE}

print("Confusion matrix at best threshold value")
cm <- table(test.data$Blue_Tarp, preds$posterior[,2] > bestThresholdValue)
cm
# Calculate accuracy
accuracy <- sum(diag(cm))/sum(cm)
# Calculate error rate
error_rate <- 1 - accuracy

# Extract the TRUE positives (TP), false positives (FP), TRUE negatives (TN), and false negatives (FN) from the confusion matrix
TP <- cm[2,2]
FP <- cm[1,2]
TN <- cm[1,1]
FN <- cm[2,1]

# Calculate the TRUE positive rate (TPR)
TPR <- TP / (TP + FN)

# Calculate the false positive rate (FPR)
FPR <- FP / (FP + TN)

# Calculate precision
precision <- TP / (TP + FP)

#Putting TPR,FPR, aucValue, bestThresholdValue, precision, and accuracy into a dataframe

logResults <- data.frame(
  TPR = TPR,
  FPR = FPR,
  AUC = aucValue,
  Best_Threshold_Value = bestThresholdValue,
  Precision = precision,
  Max_Accuracy = accuracy,
  Mean_Accuracy = meanAccuracy
)

print(t(logResults))
```

## QDA (Quadratic Discriminant Analysis)

#### Training and Cross Validation

```{r, cache = TRUE, echo = TRUE}


# Define cross-validation control
ctrl <- trainControl(method = "cv", number = 10, returnResamp = "all")

# Train model with cross-validation
model <- train(Blue_Tarp ~ Red + Blue + Green, data = train.data, method = "qda", trControl = ctrl, metric = "Accuracy")

# Print model results
print(model$finalModel)

accuracies <- model$resample$Accuracy

# Create a bar chart of the accuracies
barplot(accuracies, main = "Model Accuracies", xlab = "Resampling Iteration", ylab = "Accuracy")

meanAccuracy <- mean(accuracies)
cat("Mean accuracy over all cross-validation folds:", meanAccuracy, "\n")

```

#### Training and Cross-Validation

```{r, cache = TRUE, echo = TRUE}

#Fit a logistic regression model and do cv on it to see if it overfitting or not.

#Modify the data to add a new column for Blue Tarp

data$Blue_Tarp <- as.factor(ifelse(data$Class == "Blue Tarp", 1, 0))

#Splitting data for a training set and a holdout set
train.index <- sample(1:nrow(data), 0.8*nrow(data), replace = FALSE)
train.data <- data[train.index, ]
test.data <- data[-train.index, ]

#Set a seed
set.seed(123)

# Define cross-validation control
ctrl <- trainControl(method = "cv", number = 10, returnResamp = "all")

# Train model with cross-validation
model <- train(Blue_Tarp ~ Red + Blue + Green, data = train.data, method = "qda", trControl = ctrl, metric = "Accuracy")

# Print model results
print(model$finalModel)

accuracies <- model$resample$Accuracy

# Create a bar chart of the accuracies
barplot(accuracies, main = "Model Accuracies", xlab = "Resampling Iteration", ylab = "Accuracy")

meanAccuracy <- mean(accuracies)
cat("Mean accuracy over all cross-validation folds:", meanAccuracy, "\n")

```

------------------------------------------------------------------------

**Comment:** When plotting the accuracy of the model over each cross-validation fold, we see that the accuracy does not change significantly. This means that the model has not been over fit to any specific fold. The mean accuracy over all the folds was 99.5%.

#### ROC Curve

```{r, cache = TRUE, echo = TRUE}

# Load the MASS library
library(MASS)


# Fit the LDA model using the training data
model_training <- qda(Blue_Tarp ~ Red + Blue + Green, data = train.data)

# Make predictions on the test data
predictions_test <- predict(model_training, newdata = test.data, type = "response")
predictions_class <- predictions_test$class

# Create the confusion matrix
Actual <- test.data$Blue_Tarp
Predicted <- predictions_class
table <- table(Predicted, Actual)

# Print the confusion matrix
cat("Confusion Matrix:\n\n")
print(table)

# Calculate the overall fraction of correct predictions
accuracy <- sum(diag(table))/sum(table)

# Print the accuracy
cat("\nAccuracy:", accuracy, "\n")

preds <- predict(model_training, newdata = test.data, type = "response")

prediction_obj <- prediction(preds$posterior[,2], test.data$Blue_Tarp)

roc_obj <- performance(prediction_obj, measure = "tpr", x.measure = "fpr")
plot(roc_obj, main="ROC Curve for Blue Tarp Classification")
lines(x = c(0,1), y = c(0,1), col="red")
```

**Comment:** Since the ROC curve for this model is far above the diagonal line, this means the model performs much better than random guessing and is thus a useful model.

#### AUC Value

```{r, cache = TRUE, echo = TRUE}

auc <- performance(prediction_obj, measure = "auc")

aucValue <- as.numeric(auc@y.values[1])

cat("AUC:", aucValue, "\n")

```

**Comment:** Since the AUC of this model is 0.999 and is much greater than 0.5, this model performs much better than random guessing.

#### Confusion Matrix

```{r, cache = TRUE, echo = TRUE}

modelAccuracies <- numeric(length = 11)

for (i in 1:11) {
  x <- (i - 1) / 10
  cm <- table(test.data$Blue_Tarp, preds$posterior[,2] > x)
  modelAccuracies[i] <- sum(diag(cm))/sum(cm)
}

plot(modelAccuracies, main = "Model Accuracies at Different Thresholds", xlab = "Threshold Value", ylab = "Accuracy")

max_idx <- which.max(modelAccuracies)
bestThresholdValue <- (max_idx - 1) / 10
```

**Comment:** Threshold selection was done on the model with increments of 0.1 going from 0 to 1. The highest accuracy was obtained with a threshold value of 0.2.

#### Confusion Matrix and Best Threshold Metrics

```{r, cache = TRUE, echo = TRUE}

print("Confusion matrix at best threshold value")
cm <- table(test.data$Blue_Tarp, preds$posterior[,2] > bestThresholdValue)
cm
# Calculate accuracy
accuracy <- sum(diag(cm))/sum(cm)
# Calculate error rate
error_rate <- 1 - accuracy

# Print accuracy and error rate
cat("\n Accuracy: ", accuracy, "\n")
cat("Error rate: ", round(error_rate, 10), "\n")

# Extract the TRUE positives (TP), false positives (FP), TRUE negatives (TN), and false negatives (FN) from the confusion matrix
TP <- cm[2,2]
FP <- cm[1,2]
TN <- cm[1,1]
FN <- cm[2,1]

# Calculate the TRUE positive rate (TPR)
TPR <- TP / (TP + FN)

# Calculate the false positive rate (FPR)
FPR <- FP / (FP + TN)

# Calculate precision
precision <- TP / (TP + FP)

# Print the precision
cat("Precision:", precision, "\n")

# Print the TPR and FPR
cat("TRUE Positive Rate (TPR):", TPR, "\n")
cat("False Positive Rate (FPR):", FPR, "\n")


#Putting TPR,FPR, aucValue, bestThresholdValue, precision, and accuracy into a dataframe

logResults <- data.frame(
  TPR = TPR,
  FPR = FPR,
  AUC = aucValue,
  Best_Threshold_Value = bestThresholdValue,
  Precision = precision,
  Max_Accuracy = accuracy,
  Mean_Accuracy = meanAccuracy
)

print(t(logResults))


```

## KNN (K-nearest neighbor)

```{r, echo = TRUE, cache = TRUE}


# Modify the data to add a new column for Blue Tarp
data$Blue_Tarp <- as.factor(ifelse(data$Class == "Blue Tarp", "Yes", "No"))

set.seed(123)

# Create cross-validation folds
folds <- createFolds(data$Blue_Tarp, k = 5)

# Define cross-validation control
ctrl <- trainControl(method = "cv", index = folds)

# Define hyperparameter grid
k_values <- seq(1, 5, by = 1)
tune_grid <- expand.grid(k = k_values)

# Train model with cross-validation using KNN
model <- train(Blue_Tarp ~ Red + Blue + Green, data = data, method = "knn",
               tuneGrid = tune_grid, trControl = ctrl)

# Plot results
ggplot(data = model$results, aes(x = k, y = Accuracy)) +
  geom_line() +
  geom_point() +
 
   labs(x = "k", y = "Accuracy")


```

#### ROC Curve

```{r, cache = TRUE, echo = TRUE}
library(ROCR)
# Make predictions on test set
predictions <- predict(model, newdata = test.data, type = "prob")

# Extract predicted probabilities for class "Yes" (i.e., Blue Tarp)
predictions <- predictions[, "Yes"]

# Create a ROC curve
roc_obj <- prediction(predictions, test.data$Blue_Tarp)
roc_curve <- performance(roc_obj, "tpr", "fpr")

# Plot ROC curve
plot(roc_curve, main = "ROC Curve", colorize = TRUE, print.cutoffs.at = seq(0, 1, 0.1),
     text.adj = c(-0.2, 1.7))
abline(a = 0, b = 1, lty = 2)

```

**Comment:** Since the ROC curve for this model is far above the diagonal line, this means the model performs much better than random guessing and is thus a useful model.

#### AUC Value

```{r, cache = TRUE, echo = TRUE}

auc <- performance(roc_obj, measure = "auc")

aucValue <- as.numeric(auc@y.values[1])

cat("AUC:", aucValue, "\n")

```

**Comment:** Since the AUC of this model is 0.997 and is much greater than 0.5, this model performs much better than random guessing.

#### Confusion Matrix

```{r, cache = TRUE, echo = TRUE}

modelAccuracies <- numeric(length = 11)

for (i in 1:11) {
  x <- (i - 1) / 10
  cm <- table(test.data$Blue_Tarp, predictions > x)
  modelAccuracies[i] <- sum(diag(cm))/sum(cm)
}

plot(modelAccuracies, main = "Model Accuracies at Different Thresholds", xlab = "Threshold Value", ylab = "Accuracy")

max_idx <- which.max(modelAccuracies)
bestThresholdValue <- (max_idx - 1) / 10
```

**Comment:** Threshold selection was done on the model with increments of 0.1 going from 0 to 1. The highest accuracy was obtained with a threshold value of 0.3 at an accuracy of 0.995. Threshold selection was applied to the estimated probabilities.

#### Confusion Matrix and Best Threshold Metrics

```{r, cache = TRUE, echo = TRUE}

print("Confusion matrix at best threshold value")
cm <- table(test.data$Blue_Tarp, preds$posterior[,2] > bestThresholdValue)
cm
# Calculate accuracy
accuracy <- sum(diag(cm))/sum(cm)
# Calculate error rate
error_rate <- 1 - accuracy

# Print accuracy and error rate
cat("\n Accuracy: ", accuracy, "\n")
cat("Error rate: ", round(error_rate, 10), "\n")

# Extract the TRUE positives (TP), false positives (FP), TRUE negatives (TN), and false negatives (FN) from the confusion matrix
TP <- cm[2,2]
FP <- cm[1,2]
TN <- cm[1,1]
FN <- cm[2,1]

# Calculate the TRUE positive rate (TPR)
TPR <- TP / (TP + FN)

# Calculate the false positive rate (FPR)
FPR <- FP / (FP + TN)

# Calculate precision
precision <- TP / (TP + FP)

# Print the precision
cat("Precision:", precision, "\n")

# Print the TPR and FPR
cat("TRUE Positive Rate (TPR):", TPR, "\n")
cat("False Positive Rate (FPR):", FPR, "\n")


#Putting TPR,FPR, aucValue, bestThresholdValue, precision, and accuracy into a dataframe

logResults1 <- data.frame(
  TPR = TPR,
  FPR = FPR,
  AUC = aucValue,
  Best_Threshold_Value = bestThresholdValue,
  Precision = precision,
  Max_Accuracy = accuracy,
  Mean_Accuracy = meanAccuracy
)

print(t(logResults1))

```

## Penalized Logistic Regression Lasso Regression

### Training and Cross-Validation

```{r, cache = TRUE, echo = TRUE}

#Define the grid of tuning parameters to explore
lambdas <- c(0.1, 1, 10)  # note logarithmic scale

tuneGrid <- expand.grid(alpha=1, # select L1 regularization (Lasso)
                        lambda=lambdas)

trControl <- caret::trainControl(method = "cv", number = 10, returnResamp = "all", allowParallel=TRUE)

modelFit <- train(Blue_Tarp ~ ., 
                  data= data, 
                  method='glmnet', 
                  trControl=trControl, 
                  tuneGrid=tuneGrid,
                  metric = "Accuracy")

modelFit

# Print model results
print(model$finalModel)

accuracies <- model$resample$Accuracy

# Create a bar chart of the accuracies
barplot(accuracies, main = "Model Accuracies", xlab = "Resampling Iteration", ylab = "Accuracy")

meanAccuracy <- mean(accuracies)
cat("Mean accuracy over all cross-validation folds:", meanAccuracy, "\n")

```

#### ROC Curve

```{r, cache = TRUE, echo = TRUE}


# Make predictions on test set
predictions <- predict(modelFit, newdata = test.data, type = "prob")

# Extract predicted probabilities for class "Yes" (i.e., Blue Tarp)
predictions <- predictions[, "Yes"]

# Create a ROC curve
roc_obj <- prediction(predictions, test.data$Blue_Tarp)
roc_curve <- performance(roc_obj, "tpr", "fpr")

# Plot ROC curve
plot(roc_curve, main = "ROC Curve", colorize = TRUE, print.cutoffs.at = seq(0, 1, 0.1),
     text.adj = c(-0.2, 1.7))
abline(a = 0, b = 1, lty = 2)
```

#### AUC Value

```{r, cache = TRUE, echo = TRUE}

auc <- performance(roc_obj, measure = "auc")

aucValue <- as.numeric(auc@y.values[1])

cat("AUC:", aucValue, "\n")
```

#### Confusion Matrix

```{r, cache = TRUE, echo = TRUE}

modelAccuracies <- numeric(length = 11)

for (i in 1:11) {
  x <- (i - 1) / 10
  cm <- table(test.data$Blue_Tarp, predictions > x)
  modelAccuracies[i] <- sum(diag(cm))/sum(cm)
}

plot(modelAccuracies, main = "Model Accuracies at Different Thresholds", xlab = "Threshold Value", ylab = "Accuracy")

max_idx <- which.max(modelAccuracies)
bestThresholdValue <- (max_idx - 1) / 10
```

## Ridge Regression

```{r, cache = TRUE, echo = TRUE}

lambdas <- c(0.1, 1, 10)  # note logarithmic scale

tuneGrid <- expand.grid(alpha=0, # select L2 regularization (Ridge)
                        lambda=lambdas)

trControl <- caret::trainControl(method = "cv", number = 10, returnResamp = "all", allowParallel=TRUE)

modelFit <- train(Blue_Tarp ~ ., 
                  data= data, 
                  method='glmnet', 
                  trControl=trControl, 
                  tuneGrid=tuneGrid,
                  metric = "Accuracy")
modelFit
```

#### 

```{r, cache = TRUE, echo = TRUE}

# Print model results
print(model$finalModel)

accuracies <- model$resample$Accuracy

# Create a bar chart of the accuracies
barplot(accuracies, main = "Model Accuracies", xlab = "Resampling Iteration", ylab = "Accuracy")

meanAccuracy <- mean(accuracies)
cat("Mean accuracy over all cross-validation folds:", meanAccuracy, "\n")

```

------------------------------------------------------------------------

**Comment:** When plotting the accuracy of the model over each cross-validation fold, we see that the accuracy does not change significantly. This means that the model has not been over fit to any specific fold. The mean accuracy over all the folds was 99.5%.

#### ROC Curve

```{r, cache = TRUE, echo = TRUE}

# Make predictions on test set
predictions <- predict(modelFit, newdata = test.data, type = "prob")

# Extract predicted probabilities for class "Yes" (i.e., Blue Tarp)
predictions <- predictions[, "Yes"]

# Create a ROC curve
roc_obj <- prediction(predictions, test.data$Blue_Tarp)
roc_curve <- performance(roc_obj, "tpr", "fpr")

# Plot ROC curve
plot(roc_curve, main = "ROC Curve", colorize = TRUE, print.cutoffs.at = seq(0, 1, 0.1),
     text.adj = c(-0.2, 1.7))
abline(a = 0, b = 1, lty = 2)
```

**Comment:** Since the ROC curve for this model is far above the diagonal line, this means the model performs much better than random guessing and is thus a useful model.

#### AUC Value

```{r, cache = TRUE, echo = TRUE}

auc <- performance(roc_obj, measure = "auc")

aucValue <- as.numeric(auc@y.values[1])

cat("AUC:", aucValue, "\n")

```

**Comment:** Since the AUC of this model is 0.999 and is much greater than 0.5, this model performs much better than random guessing.

#### Confusion Matrix

```{r, cache = TRUE, echo = TRUE}

modelAccuracies <- numeric(length = 11)

for (i in 1:11) {
  x <- (i - 1) / 10
  cm <- table(test.data$Blue_Tarp, predictions > x)
  modelAccuracies[i] <- sum(diag(cm))/sum(cm)
}

plot(modelAccuracies, main = "Model Accuracies at Different Thresholds", xlab = "Threshold Value", ylab = "Accuracy")

max_idx <- which.max(modelAccuracies)
bestThresholdValue <- (max_idx - 1) / 10
```

**Comment:** Threshold selection was done on the model with increments of 0.1 going from 0 to 1. The highest accuracy was obtained with a threshold value of 0.3 at an accuracy of 0.995. Threshold selection was applied to the estimated probabilities.

## Conclusions

### Conclusion 1

```{r, cache = TRUE, echo = TRUE}
print(logResults1)
```

Based on the output metrics, it can be concluded that the KNN model performed best at k = 5. The TRUE positive rate (TPR) of the model was relatively high at 0.825, indicating that the model was able to correctly identify a large proportion of positive instances. Additionally, the false positive rate (FPR) was extremely low at 0.0006, indicating that the model was able to avoid making false positive predictions.

The area under the ROC curve (AUC) was also high at 0.918, indicating that the model's predictions were overall accurate and well-balanced. Furthermore, the precision of the model was high at 0.98, indicating that the model was precise in identifying positive instances.

The max accuracy of the model was 0.966, which is relatively high, indicating that the model was able to achieve a high level of accuracy. Additionally, the mean accuracy of the model was even higher at 0.996, indicating that the model was able to maintain a high level of accuracy across different data sets.

Overall, these output metrics suggest that the KNN model performed well at k = 5 and was able to accurately classify positive instances while minimizing false positives. Therefore, it can be concluded that the KNN model was the best model for this particular data set and problem.

### Conclusion 2

Some additional recommended actions that can be taken to improve results over classification methods are that convolution neural network (CNN) models can be used to identify the images with the Blue Tarps in them. This may prove more beneficial as CNNs are specifically designed for image recognition tasks and have shown exceptional performance in image classification tasks. Compared to traditional classification methods, CNNs can automatically learn the important features from images and extract the most relevant information to make accurate predictions.

Moreover, when dealing with large datasets or complex images, deep learning models such as CNNs can be used to capture the underlying patterns and relationships in the data, which may be difficult to detect using traditional machine learning methods. Thus, using CNN models may prove to be more useful when identifying survivors from this data set.

### Conclusion 3

One reason why the models trained were able to attain such a high accuracy may have been due to the uniqueness of the Blue Tarp pixels. When analyzing the visualizations of these models, we see that the Blue Tarp class has the distribution of Blue color on the highest end as compared to the other classes. This may add to the uniqueness of the Blue Tarp class which allows the models to perform very accurately. Furthermore, we can also see from the visualizations that the Blue Tarp class pixels are the least compared to the other classes. Due to its rareness in the data set, the model learns very well what the pixels without the Blue Tarp in it look like and is thus able to dismiss them accurately leading to better model over all. This observation tells us that in this case of the Haiti disaster, it may make sense to use these types of classification models to identify where people are living because of the color of the tents they are using. However, if the tents that survivors are using are a different color, it may prove more difficult for these algorithms to identify the survivors properly.
